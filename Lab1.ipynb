{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¬ Linear Regression: Closed-Form vs Gradient Descent\n",
        "\n",
        "This notebook accompanies the problem set from **Session 1** of *Decode Life 2025*.\n",
        "\n",
        "## Goals:\n",
        "- Generate synthetic linear data with small noise.\n",
        "- Fit a line using **Closed Form Equation** and **Gradient Descent**.\n",
        "- Compare the two methods in low-data vs. high-data regimes.\n",
        "\n",
        "## Model:\n",
        "We assume the model:\n",
        "\\begin{align}\n",
        "y = wx + b + \\text{noise}\n",
        "\\end{align}\n",
        "\n",
        "This can be rewritten in matrix form as:\n",
        "\\begin{align}\n",
        "\\mathbf{y} = X \\boldsymbol{\\theta}, \\quad \\text{where } X =\n",
        "\\begin{bmatrix}\n",
        "x_1 & 1 \\\\\n",
        "x_2 & 1 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_n & 1\n",
        "\\end{bmatrix}, \\quad\n",
        "\\boldsymbol{\\theta} =\n",
        "\\begin{bmatrix}\n",
        "w \\\\\n",
        "b\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Our task is to minimize the total squared error:\n",
        "\\begin{align}\n",
        "\\min_{\\boldsymbol{\\theta}} \\| X\\boldsymbol{\\theta} - \\mathbf{y} \\|^2\n",
        "\\end{align}\n",
        "\n",
        "We will do this using:\n",
        "1. The Closed Form equation: \\begin{align}\n",
        " \\boldsymbol{\\theta}^* = (X^\\top X)^{-1} X^\\top \\mathbf{y} \\end{align}\n",
        "2. Gradient descent:\n",
        "\\begin{align}\n",
        "w \\leftarrow w - \\eta \\frac{\\partial J}{\\partial w}, \\quad b \\leftarrow b - \\eta \\frac{\\partial J}{\\partial b}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "UDDpCM9DD3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Parameters\n",
        "n_points = 100\n",
        "x = np.linspace(0, 10, n_points)\n",
        "true_w = 3\n",
        "true_b = 2\n",
        "noise = np.random.normal(0, 1, size=n_points)\n",
        "\n",
        "# Generate y\n",
        "y = true_w * x + true_b + noise\n",
        "\n",
        "# Save to CSV\n",
        "data = pd.DataFrame({'x': x, 'y': y})\n",
        "data.to_csv(\"linear_regression_data.csv\", index=False)\n",
        "\n",
        "print(\"Data saved to linear_regression_data.csv\")"
      ],
      "metadata": {
        "id": "dynSfsEkE4xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression: Closed-Form vs Gradient Descent\n",
        "\n",
        "This notebook explores two methods for fitting a line to data:\n",
        "\n",
        "- Closed-form solution using the normal equation\n",
        "- Gradient descent using calculus-based optimization\n",
        "\n",
        "We will compare both methods under two settings:\n",
        "- Low-dimensional regime (few data points, small feature range)\n",
        "- High-dimensional regime (many data points, wide feature range)\n",
        "\n",
        "## Step 1: Load the Data\n",
        "\n",
        "First, load the synthetic data we generated. The file should contain two columns: `x` and `y`.\n"
      ],
      "metadata": {
        "id": "B7xN1qA3Fl3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"linear_regression_data.csv\")\n",
        "x = data['x'].values\n",
        "y = data['y'].values\n",
        "\n",
        "plt.scatter(x, y, alpha=0.6)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Synthetic Linear Data\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qpazABW5D0AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Closed-Form Solution"
      ],
      "metadata": {
        "id": "bTowgDYsGBgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Design matrix X with bias column\n",
        "X = np.column_stack([x, np.ones_like(x)])\n",
        "\n",
        "# Find Closed-form solution\n",
        "\n",
        "## TO DO\n"
      ],
      "metadata": {
        "id": "IRJ9yz_FD0Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Gradient Descent Method"
      ],
      "metadata": {
        "id": "0o1vRfJVGcFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient descent implementation\n",
        "def gradient_descent(x, y, eta=0.001, steps=1000):\n",
        "    w, b = 0.0, 0.0\n",
        "    n = len(x)\n",
        "\n",
        "## TO DO\n",
        "\n",
        "    return w, b\n",
        "w_gd, b_gd = gradient_descent(x, y, eta=0.001, steps=10000)\n",
        "print(f\"Gradient Descent solution: w = {w_gd:.3f}, b = {b_gd:.3f}\")"
      ],
      "metadata": {
        "id": "qq_2m9OoGZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Compare Predictions\n"
      ],
      "metadata": {
        "id": "-wggeexEOipZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TO DO\n",
        "plt.scatter(x, y, label=\"Data\", alpha=0.6)\n",
        "plt.plot(x, w_closed * x + b_closed, label=\"Closed-form\", linewidth=2)\n",
        "plt.plot(x, w_gd * x + b_gd, label=\"Gradient Descent\", linestyle=\"--\", linewidth=2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title(\"Line Fit Comparison\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9jQARijHOojI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}